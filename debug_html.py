"""
HTMLè°ƒè¯•å·¥å…· - ç”¨äºæŸ¥çœ‹ç½‘ç«™HTMLç»“æ„ï¼Œåˆ¶å®šçˆ¬å–è§„åˆ™
"""
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import sys


def fetch_and_display_html(url, save_to_file=True):
    """
    è·å–å¹¶æ˜¾ç¤ºç½‘é¡µHTMLå†…å®¹
    
    Args:
        url: ç›®æ ‡URL
        save_to_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
    """
    print("=" * 70)
    print(f"æ­£åœ¨è·å–ç½‘é¡µ: {url}")
    print("=" * 70)
    
    try:
        # è®¾ç½®è¯·æ±‚å¤´
        ua = UserAgent()
        headers = {
            'User-Agent': ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        # å‘é€è¯·æ±‚
        print("\n>>> å‘é€HTTPè¯·æ±‚...")
        response = requests.get(url, headers=headers, timeout=30)
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        
        print(f"âœ… çŠ¶æ€ç : {response.status_code}")
        print(f"âœ… ç¼–ç : {response.encoding}")
        print(f"âœ… å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'lxml')
        
        # ç¾åŒ–HTML
        pretty_html = soup.prettify()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            filename = 'data/html_debug.html'
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(pretty_html)
            print(f"\nâœ… HTMLå·²ä¿å­˜åˆ°: {filename}")
        
        # æ‰“å°HTMLç»“æ„æ‘˜è¦
        print("\n" + "=" * 70)
        print("HTMLç»“æ„æ‘˜è¦:")
        print("=" * 70)
        
        # 1. æ‰“å°title
        if soup.title:
            print(f"\nğŸ“Œ ç½‘é¡µæ ‡é¢˜: {soup.title.string}")
        
        # 2. æŸ¥æ‰¾å¸¸è§çš„æ¯”èµ›åˆ—è¡¨å®¹å™¨
        print("\nğŸ“Œ æŸ¥æ‰¾å¯èƒ½çš„æ¯”èµ›åˆ—è¡¨å®¹å™¨:")
        print("-" * 70)
        
        # å¸¸è§çš„classåç§°
        common_classes = [
            'match', 'game', 'event', 'fixture', 'contest',
            'match-item', 'match-list', 'match-row', 'match-box',
            'game-item', 'game-list', 'game-row',
            'list-item', 'table-row', 'data-row'
        ]
        
        found_containers = []
        for class_name in common_classes:
            # æŸ¥æ‰¾åŒ…å«è¿™äº›classçš„å…ƒç´ 
            elements = soup.find_all(class_=lambda x: x and class_name in x.lower())
            if elements:
                for elem in elements[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    found_containers.append({
                        'tag': elem.name,
                        'class': elem.get('class'),
                        'id': elem.get('id'),
                        'text_preview': elem.get_text(strip=True)[:100]
                    })
        
        if found_containers:
            for i, container in enumerate(found_containers[:10], 1):
                print(f"\nå®¹å™¨ {i}:")
                print(f"  æ ‡ç­¾: <{container['tag']}>")
                print(f"  class: {container['class']}")
                if container['id']:
                    print(f"  id: {container['id']}")
                print(f"  å†…å®¹é¢„è§ˆ: {container['text_preview']}...")
        else:
            print("  æœªæ‰¾åˆ°å¸¸è§çš„æ¯”èµ›åˆ—è¡¨å®¹å™¨ï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹HTMLæ–‡ä»¶")
        
        # 3. æŸ¥æ‰¾è¡¨æ ¼
        print("\nğŸ“Œ æŸ¥æ‰¾è¡¨æ ¼ (table):")
        print("-" * 70)
        tables = soup.find_all('table')
        if tables:
            print(f"  æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            for i, table in enumerate(tables[:3], 1):
                print(f"\n  è¡¨æ ¼ {i}:")
                print(f"    class: {table.get('class')}")
                print(f"    id: {table.get('id')}")
                # æ˜¾ç¤ºè¡¨å¤´
                thead = table.find('thead')
                if thead:
                    headers = [th.get_text(strip=True) for th in thead.find_all('th')]
                    print(f"    è¡¨å¤´: {headers}")
        else:
            print("  æœªæ‰¾åˆ°è¡¨æ ¼")
        
        # 4. æ˜¾ç¤ºéƒ¨åˆ†HTMLï¼ˆå‰1000å­—ç¬¦ï¼‰
        print("\n" + "=" * 70)
        print("HTMLå†…å®¹é¢„è§ˆï¼ˆå‰1000å­—ç¬¦ï¼‰:")
        print("=" * 70)
        print(pretty_html[:1000])
        print("\n... (æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹æ–‡ä»¶: data/html_debug.html)")
        
        # 5. æä¾›å»ºè®®
        print("\n" + "=" * 70)
        print("ğŸ“ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("=" * 70)
        print("1. æ‰“å¼€æµè§ˆå™¨è®¿é—®è¯¥ç½‘å€ï¼ŒæŒ‰F12æŸ¥çœ‹å¼€å‘è€…å·¥å…·")
        print("2. æŸ¥çœ‹ç”Ÿæˆçš„ data/html_debug.html æ–‡ä»¶")
        print("3. åœ¨æµè§ˆå™¨ä¸­æ‰¾åˆ°æ¯”èµ›åˆ—è¡¨çš„HTMLç»“æ„")
        print("4. è®°å½•å…³é”®çš„æ ‡ç­¾ã€classã€idç­‰é€‰æ‹©å™¨")
        print("5. ä¿®æ”¹ crawler.py ä¸­çš„è§£æè§„åˆ™")
        print("=" * 70)
        
        return response.text
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("HTMLè°ƒè¯•å·¥å…· - åˆ†æç½‘ç«™ç»“æ„")
    print("=" * 70)
    
    # é¢„è®¾çš„è¶³å½©ç½‘ç«™URL
    urls = {
        '1': ('500å½©ç¥¨ç½‘', 'https://live.500.com/'),
        '2': ('ä¸­å›½è¶³å½©ç½‘', 'https://www.zgzcw.com/'),
        '3': ('æ¾³å®¢ç½‘', 'https://www.okooo.com/'),
        '4': ('è‡ªå®šä¹‰URL', None),
    }
    
    print("\nè¯·é€‰æ‹©è¦åˆ†æçš„ç½‘ç«™:")
    for key, (name, url) in urls.items():
        if url:
            print(f"  {key}. {name} - {url}")
        else:
            print(f"  {key}. {name}")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4): ").strip()
    
    if choice in urls:
        if choice == '4':
            url = input("è¯·è¾“å…¥ç½‘å€: ").strip()
        else:
            url = urls[choice][1]
        
        if url:
            fetch_and_display_html(url)
        else:
            print("âŒ æ— æ•ˆçš„URL")
    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == '__main__':
    main()
